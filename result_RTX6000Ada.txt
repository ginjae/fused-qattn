nvcc -o eval src/eval.cu src/attn_naive.cu src/attn_tiled.cu src/attn_flash.cu src/attn_ours.cu src/quantization_utils.cu -Isrc
CUDA_VISIBLE_DEVICES=0 ./eval

=== Starting Evaluation (GPT-2 Scale, Using Real GPT-2 Weights) ===

== Loading GPT-2 Weights ==
Successfully loaded GPT-2 weights from weights/
  Wq shape: (768, 64)
  Wk shape: (768, 64)
  Wv shape: (768, 64)
  bq shape: (64, 1)
  bk shape: (64, 1)
  bv shape: (64, 1)

== Initializing Weights ==
Using real GPT-2 weights
Cooling down GPU for 1 seconds...

=== Unquantized Weights Tests ===

1. Naive Attention (BASELINE)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.1434 ms
Cooling down GPU for 1 seconds...

2. Tiled Attention
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.0758 ms
  Tiled vs Naive Correctness Check:
    Max Absolute Error: 7.152557e-07
    Mean Absolute Error: 4.668544e-08
    RMSE: 9.275157e-08
    Relative Error: 9.930392e-08
    Status: ✓ PASS (High precision match)
Cooling down GPU for 1 seconds...

3. Flash-style Attention
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.0645 ms
  Flash vs Naive Correctness Check:
    Max Absolute Error: 7.748604e-06
    Mean Absolute Error: 2.833742e-07
    RMSE: 6.579477e-07
    Relative Error: 7.044278e-07
    Status: ✓ PASS (High precision match)
=== End of Test ===


=== Quantized Weights Tests (MXFP4) ===

== Quantizing Weights to MXFP4 ==
MXFP4 Block size: 32
Num MXFP4 blocks (Q/K): 1536, Num MXFP4 blocks (V): 1536
MXFP4 Quantization complete.
MXFP4 Weight Quantization Error: 1.235589e-01 (12.36%)
MXFP4 GPU vs CPU Dequantization Error: 0.000000e+00
Cooling down GPU for 1 seconds...

1. Naive Attention (MXFP4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.1475 ms
Cooling down GPU for 1 seconds...

2. Tiled Attention (MXFP4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.0973 ms
  Tiled MXFP4 vs Naive MXFP4 Correctness Check:
    Max Absolute Error: 7.152557e-07
    Mean Absolute Error: 3.782713e-08
    RMSE: 7.664710e-08
    Relative Error: 8.501847e-08
    Status: ✓ PASS (High precision match)
Cooling down GPU for 1 seconds...

3. Flash-style Attention (MXFP4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.0860 ms
  Flash MXFP4 vs Naive MXFP4 Correctness Check:
    Max Absolute Error: 1.424551e-05
    Mean Absolute Error: 3.343110e-07
    RMSE: 9.148537e-07
    Relative Error: 1.014774e-06
    Status: ✓ PASS (High precision match)
Cooling down GPU for 1 seconds...

4. Our Attention (MXFP4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.0748 ms
  Our MXFP4 vs Naive MXFP4 Correctness Check:
    Max Absolute Error: 1.424551e-05
    Mean Absolute Error: 3.343110e-07
    RMSE: 9.148537e-07
    Relative Error: 1.014774e-06
    Status: ✓ PASS (High precision match)


=== Quantized Weights Tests (NF4) ===

== Quantizing Weights to NF4 ==
NF4 Block size: 64
Num NF4 blocks (Q/K): 768, Num NF4 blocks (V): 768
NF4 Quantization complete.
NF4 Weight Quantization Error: 9.237896e-02 (9.24%)
NF4 GPU vs CPU Dequantization Error: 0.000000e+00
Cooling down GPU for 1 seconds...

1. Naive Attention (NF4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.1802 ms
Cooling down GPU for 1 seconds...

2. Tiled Attention (NF4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.1137 ms
  Tiled NF4 vs Naive NF4 Correctness Check:
    Max Absolute Error: 1.907349e-06
    Mean Absolute Error: 3.667683e-08
    RMSE: 8.491409e-08
    Relative Error: 9.171419e-08
    Status: ✓ PASS (High precision match)
Cooling down GPU for 1 seconds...

3. Flash-style Attention (NF4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.0985 ms
  Flash NF4 vs Naive NF4 Correctness Check:
    Max Absolute Error: 1.087785e-05
    Mean Absolute Error: 4.049066e-07
    RMSE: 9.560685e-07
    Relative Error: 1.032633e-06
    Status: ✓ PASS (High precision match)
Cooling down GPU for 1 seconds...

4. Our Attention (NF4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.0889 ms
  Our NF4 vs Naive NF4 Correctness Check:
    Max Absolute Error: 1.087785e-05
    Mean Absolute Error: 4.049066e-07
    RMSE: 9.560685e-07
    Relative Error: 1.032633e-06
    Status: ✓ PASS (High precision match)


========================================================================
                         PERFORMANCE SUMMARY                            
========================================================================
Implementation       | No Quant (ms) | MXFP4 (ms)   | NF4 (ms)    
------------------------------------------------------------------------
Naive                |        0.1434 |       0.1475 |       0.1802
Tiled                |        0.0758 |       0.0973 |       0.1137
Flash                |        0.0645 |       0.0860 |       0.0985
Ours                 |           N/A |       0.0748 |       0.0889
========================================================================

========================================================================
                    SPEEDUP vs Naive (Same Quant)                      
========================================================================
Implementation       | No Quant      | MXFP4        | NF4         
------------------------------------------------------------------------
Tiled                |         1.89x |        1.52x |        1.59x
Flash                |         2.22x |        1.72x |        1.83x
Ours                 |           N/A |        1.97x |        2.03x
========================================================================

=== End of Test ===
