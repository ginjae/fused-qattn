CUDA_VISIBLE_DEVICES=0 ./eval

=== Starting Evaluation (GPT-2 Scale, Using Real GPT-2 Weights) ===

== Loading GPT-2 Weights ==
Successfully loaded GPT-2 weights from weights/
  Wq shape: (768, 64)
  Wk shape: (768, 64)
  Wv shape: (768, 64)
  bq shape: (64, 1)
  bk shape: (64, 1)
  bv shape: (64, 1)

== Initializing Weights ==
Using real GPT-2 weights
Cooling down GPU for 1 seconds...

=== Unquantized Weights Tests ===

1. Naive Attention (BASELINE)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.2171 ms
Cooling down GPU for 1 seconds...

2. Tiled Attention
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.1382 ms
  Tiled vs Naive Correctness Check:
    Max Absolute Error: 7.152557e-07
    Mean Absolute Error: 4.668544e-08
    RMSE: 9.275157e-08
    Relative Error: 9.930392e-08
    Status: ✓ PASS (High precision match)
Cooling down GPU for 1 seconds...

3. Flash-style Attention
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.1167 ms
  Flash vs Naive Correctness Check:
    Max Absolute Error: 7.748604e-06
    Mean Absolute Error: 2.833742e-07
    RMSE: 6.579477e-07
    Relative Error: 7.044278e-07
    Status: ✓ PASS (High precision match)
=== End of Test ===


=== Quantized Weights Tests (MXFP4) ===

== Quantizing Weights to MXFP4 ==
MXFP4 Block size: 32
Num MXFP4 blocks (Q/K): 1536, Num MXFP4 blocks (V): 1536
MXFP4 Quantization complete.
MXFP4 Weight Quantization Error: 1.235589e-01 (12.36%)
MXFP4 GPU vs CPU Dequantization Error: 0.000000e+00
Cooling down GPU for 1 seconds...

1. Naive Attention (MXFP4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.2673 ms
Cooling down GPU for 1 seconds...

2. Tiled Attention (MXFP4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.1853 ms
  Tiled MXFP4 vs Naive MXFP4 Correctness Check:
    Max Absolute Error: 7.152557e-07
    Mean Absolute Error: 3.782713e-08
    RMSE: 7.664710e-08
    Relative Error: 8.501847e-08
    Status: ✓ PASS (High precision match)
Cooling down GPU for 1 seconds...

3. Flash-style Attention (MXFP4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.1608 ms
  Flash MXFP4 vs Naive MXFP4 Correctness Check:
    Max Absolute Error: 1.424551e-05
    Mean Absolute Error: 3.343110e-07
    RMSE: 9.148537e-07
    Relative Error: 1.014774e-06
    Status: ✓ PASS (High precision match)
Cooling down GPU for 1 seconds...

4. Our Attention (MXFP4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.1239 ms
  Our MXFP4 vs Naive MXFP4 Correctness Check:
    Max Absolute Error: 1.424551e-05
    Mean Absolute Error: 3.343110e-07
    RMSE: 9.148537e-07
    Relative Error: 1.014774e-06
    Status: ✓ PASS (High precision match)


=== Quantized Weights Tests (NF4) ===

== Quantizing Weights to NF4 ==
NF4 Block size: 64
Num NF4 blocks (Q/K): 768, Num NF4 blocks (V): 768
NF4 Quantization complete.
NF4 Weight Quantization Error: 9.237896e-02 (9.24%)
NF4 GPU vs CPU Dequantization Error: 0.000000e+00
Cooling down GPU for 1 seconds...

1. Naive Attention (NF4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.2509 ms
Cooling down GPU for 1 seconds...

2. Tiled Attention (NF4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.1853 ms
  Tiled NF4 vs Naive NF4 Correctness Check:
    Max Absolute Error: 1.907349e-06
    Mean Absolute Error: 3.667683e-08
    RMSE: 8.491409e-08
    Relative Error: 9.171419e-08
    Status: ✓ PASS (High precision match)
Cooling down GPU for 1 seconds...

3. Flash-style Attention (NF4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.1618 ms
  Flash NF4 vs Naive NF4 Correctness Check:
    Max Absolute Error: 1.087785e-05
    Mean Absolute Error: 4.049066e-07
    RMSE: 9.560685e-07
    Relative Error: 1.032633e-06
    Status: ✓ PASS (High precision match)
Cooling down GPU for 1 seconds...

4. Our Attention (NF4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.1362 ms
  Our NF4 vs Naive NF4 Correctness Check:
    Max Absolute Error: 1.087785e-05
    Mean Absolute Error: 4.049066e-07
    RMSE: 9.560685e-07
    Relative Error: 1.032633e-06
    Status: ✓ PASS (High precision match)


=== Quantized Weights Tests (NVFP4) ===

== Quantizing Weights to NVFP4 ==
NVFP4 Block size: 16
Num NVFP4 blocks (Q/K): 3072, Num NVFP4 blocks (V): 3072
NVFP4 Quantization complete.
NVFP4 Weight Quantization Error: 1.027644e-01 (10.28%)
NVFP4 GPU vs CPU Dequantization Error: 0.000000e+00
Cooling down GPU for 1 seconds...

1. Naive Attention (NVFP4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.2478 ms
Cooling down GPU for 1 seconds...

2. Tiled Attention (NVFP4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.1853 ms
  Tiled NVFP4 vs Naive NVFP4 Correctness Check:
    Max Absolute Error: 9.536743e-07
    Mean Absolute Error: 4.036449e-08
    RMSE: 8.913906e-08
    Relative Error: 9.716170e-08
    Status: ✓ PASS (High precision match)
Cooling down GPU for 1 seconds...

3. Flash-style Attention (NVFP4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.1618 ms
  Flash NVFP4 vs Naive NVFP4 Correctness Check:
    Max Absolute Error: 1.180172e-05
    Mean Absolute Error: 4.989612e-07
    RMSE: 1.070309e-06
    Relative Error: 1.166639e-06
    Status: ✓ PASS (High precision match)
Cooling down GPU for 1 seconds...

4. Our Attention (NVFP4)
Running dummy run for warm-up...
Running 100 iterations...
Median execution time: 0.1741 ms
  Our NVFP4 vs Naive NVFP4 Correctness Check:
    Max Absolute Error: 1.180172e-05
    Mean Absolute Error: 4.989612e-07
    RMSE: 1.070309e-06
    Relative Error: 1.166639e-06
    Status: ✓ PASS (High precision match)


======================================================================================
                                  PERFORMANCE SUMMARY                                 
======================================================================================
Implementation       | No Quant (ms) | MXFP4 (ms)   | NF4 (ms)     | NVFP4 (ms)  
--------------------------------------------------------------------------------------
Naive                |        0.2171 |       0.2673 |       0.2509 |       0.2478
Tiled                |        0.1382 |       0.1853 |       0.1853 |       0.1853
Flash                |        0.1167 |       0.1608 |       0.1618 |       0.1618
Ours                 |           N/A |       0.1239 |       0.1362 |       0.1741
======================================================================================

======================================================================================
                             SPEEDUP vs Naive (Same Quant)                            
======================================================================================
Implementation       | No Quant      | MXFP4        | NF4          | NVFP4       
--------------------------------------------------------------------------------------
Tiled                |         1.57x |        1.44x |        1.35x |        1.34x
Flash                |         1.86x |        1.66x |        1.55x |        1.53x
Ours                 |           N/A |        2.16x |        1.84x |        1.42x
======================================================================================

=== End of Test ===
